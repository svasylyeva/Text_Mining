# Text_Mining
An ability to work with text data is one of the main technological achievements: 
it helped many companies to transform free (unstructured) text into a structured format to identify some patterns 
and new insights making them able to explore hidden relationships within their data. 
One of the difficult tasks in NLP is Machine translation (MT) evaluation, 
that determines the effectiveness of the MT system in general, estimates the level of post-editing needed and sets reasonable expectations. 
Machine translation output can be evaluated automatically, using some ready metrics or by human judges.
The idea of this project is inspired by Metric Shared Task competition and the aim is to develop a metric 
that predicts the quality of a translation using the reference for six language pairs (cs-en, de-en, en-fi, en-zh, ru-en, zh-en). 
The metric should correlate well with the existing quality assessments (z-score and avg-score with a given number of annotators). 
We also evaluated quality of translation for a new text set using the developed metric.
