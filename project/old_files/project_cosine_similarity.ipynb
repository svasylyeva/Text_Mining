{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Import libraries\n",
    "import nltk.translate.chrf_score as chrf\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "import nltk.translate.gleu_score as gleu\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "try:\n",
    "  nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "  nltk.download('punkt')\n",
    "import nltk.translate.meteor_score as meteor\n",
    "import nltk.translate.nist_score as nist\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "#import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "\n",
    "import codecs\n",
    "import jieba\n",
    "\n",
    "#Import nltk Snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "  nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cs = pd.read_csv(\"corpus/cs-en/scores.csv\")\n",
    "scores_de = pd.read_csv(\"corpus/de-en/scores.csv\")\n",
    "scores_en_fi = pd.read_csv(\"corpus/en-fi/scores.csv\")\n",
    "scores_en_zh = pd.read_csv(\"corpus/en-zh/scores.csv\")\n",
    "scores_ru = pd.read_csv(\"corpus/ru-en/scores.csv\")\n",
    "scores_zh = pd.read_csv(\"corpus/zh-en/scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED - WAS LEFT HERE TO SHOW HOW CONTRACTIONS LOOK LIKE\n",
    "\n",
    "# contractions = { \n",
    "# \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "# \"aren't\": \"are not / am not\",\n",
    "# \"can't\": \"cannot\",\n",
    "# \"can't've\": \"cannot have\",\n",
    "# \"'cause\": \"because\",\n",
    "# \"could've\": \"could have\",\n",
    "# \"couldn't\": \"could not\",\n",
    "# \"couldn't've\": \"could not have\",\n",
    "# \"didn't\": \"did not\",\n",
    "# \"doesn't\": \"does not\",\n",
    "# \"don't\": \"do not\",\n",
    "# \"hadn't\": \"had not\",\n",
    "# \"hadn't've\": \"had not have\",\n",
    "# \"hasn't\": \"has not\",\n",
    "# \"haven't\": \"have not\",\n",
    "# \"he'd\": \"he had / he would\",\n",
    "# \"he'd've\": \"he would have\",\n",
    "# \"he'll\": \"he shall / he will\",\n",
    "# \"he'll've\": \"he shall have / he will have\",\n",
    "# \"he's\": \"he has / he is\",\n",
    "# \"how'd\": \"how did\",\n",
    "# \"how'd'y\": \"how do you\",\n",
    "# \"how'll\": \"how will\",\n",
    "# \"how's\": \"how has / how is / how does\",\n",
    "# \"I'd\": \"I had / I would\",\n",
    "# \"I'd've\": \"I would have\",\n",
    "# \"I'll\": \"I shall / I will\",\n",
    "# \"I'll've\": \"I shall have / I will have\",\n",
    "# \"I'm\": \"I am\",\n",
    "# \"I've\": \"I have\",\n",
    "# \"isn't\": \"is not\",\n",
    "# \"it'd\": \"it had / it would\",\n",
    "# \"it'd've\": \"it would have\",\n",
    "# \"it'll\": \"it shall / it will\",\n",
    "# \"it'll've\": \"it shall have / it will have\",\n",
    "# \"it's\": \"it has / it is\",\n",
    "# \"let's\": \"let us\",\n",
    "# \"ma'am\": \"madam\",\n",
    "# \"mayn't\": \"may not\",\n",
    "# \"might've\": \"might have\",\n",
    "# \"mightn't\": \"might not\",\n",
    "# \"mightn't've\": \"might not have\",\n",
    "# \"must've\": \"must have\",\n",
    "# \"mustn't\": \"must not\",\n",
    "# \"mustn't've\": \"must not have\",\n",
    "# \"needn't\": \"need not\",\n",
    "# \"needn't've\": \"need not have\",\n",
    "# \"o'clock\": \"of the clock\",\n",
    "# \"oughtn't\": \"ought not\",\n",
    "# \"oughtn't've\": \"ought not have\",\n",
    "# \"shan't\": \"shall not\",\n",
    "# \"sha'n't\": \"shall not\",\n",
    "# \"shan't've\": \"shall not have\",\n",
    "# \"she'd\": \"she had / she would\",\n",
    "# \"she'd've\": \"she would have\",\n",
    "# \"she'll\": \"she shall / she will\",\n",
    "# \"she'll've\": \"she shall have / she will have\",\n",
    "# \"she's\": \"she has / she is\",\n",
    "# \"should've\": \"should have\",\n",
    "# \"shouldn't\": \"should not\",\n",
    "# \"shouldn't've\": \"should not have\",\n",
    "# \"so've\": \"so have\",\n",
    "# \"so's\": \"so as / so is\",\n",
    "# \"that'd\": \"that would / that had\",\n",
    "# \"that'd've\": \"that would have\",\n",
    "# \"that's\": \"that has / that is\",\n",
    "# \"there'd\": \"there had / there would\",\n",
    "# \"there'd've\": \"there would have\",\n",
    "# \"there's\": \"there has / there is\",\n",
    "# \"they'd\": \"they had / they would\",\n",
    "# \"they'd've\": \"they would have\",\n",
    "# \"they'll\": \"they shall / they will\",\n",
    "# \"they'll've\": \"they shall have / they will have\",\n",
    "# \"they're\": \"they are\",\n",
    "# \"they've\": \"they have\",\n",
    "# \"to've\": \"to have\",\n",
    "# \"wasn't\": \"was not\",\n",
    "# \"we'd\": \"we had / we would\",\n",
    "# \"we'd've\": \"we would have\",\n",
    "# \"we'll\": \"we will\",\n",
    "# \"we'll've\": \"we will have\",\n",
    "# \"we're\": \"we are\",\n",
    "# \"we've\": \"we have\",\n",
    "# \"weren't\": \"were not\",\n",
    "# \"what'll\": \"what shall / what will\",\n",
    "# \"what'll've\": \"what shall have / what will have\",\n",
    "# \"what're\": \"what are\",\n",
    "# \"what's\": \"what has / what is\",\n",
    "# \"what've\": \"what have\",\n",
    "# \"when's\": \"when has / when is\",\n",
    "# \"when've\": \"when have\",\n",
    "# \"where'd\": \"where did\",\n",
    "# \"where's\": \"where has / where is\",\n",
    "# \"where've\": \"where have\",\n",
    "# \"who'll\": \"who shall / who will\",\n",
    "# \"who'll've\": \"who shall have / who will have\",\n",
    "# \"who's\": \"who has / who is\",\n",
    "# \"who've\": \"who have\",\n",
    "# \"why's\": \"why has / why is\",\n",
    "# \"why've\": \"why have\",\n",
    "# \"will've\": \"will have\",\n",
    "# \"won't\": \"will not\",\n",
    "# \"won't've\": \"will not have\",\n",
    "# \"would've\": \"would have\",\n",
    "# \"wouldn't\": \"would not\",\n",
    "# \"wouldn't've\": \"would not have\",\n",
    "# \"y'all\": \"you all\",\n",
    "# \"y'all'd\": \"you all would\",\n",
    "# \"y'all'd've\": \"you all would have\",\n",
    "# \"y'all're\": \"you all are\",\n",
    "# \"y'all've\": \"you all have\",\n",
    "# \"you'd\": \"you had / you would\",\n",
    "# \"you'd've\": \"you would have\",\n",
    "# \"you'll\": \"you shall / you will\",\n",
    "# \"you'll've\": \"you shall have / you will have\",\n",
    "# \"you're\": \"you are\",\n",
    "# \"you've\": \"you have\"\n",
    "# }\n",
    "\n",
    "# contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "# def expand_contractions(s, contractions = contractions):\n",
    "#     def replace(match):\n",
    "#         return contractions[match.group(0)]\n",
    "#     return contractions_re.sub(replace, s)\n",
    "\n",
    "# expand_contractions(\"ain't stop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with contractions as previous cell but pycontractes lib chooses a proper form\n",
    "# instead ambiguous \"you'd\" -> \"you had / you would\"\n",
    "\n",
    "from pycontractions import Contractions\n",
    "cont = Contractions('GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# prevents loading on first expand_texts call\n",
    "# usually takes time\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = scores_de.translation\n",
    "a[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.Series(list(cont.expand_texts(a, precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how contracion expanding worked\n",
    "b[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As contraction extractions work really slow\n",
    "# they were done outside preprocessing function\n",
    "references_cs = pd.Series(list(cont.expand_texts(scores_cs['reference'], precise=True)))\n",
    "references_de = pd.Series(list(cont.expand_texts(scores_de['reference'], precise=True)))\n",
    "references_ru = pd.Series(list(cont.expand_texts(scores_ru['reference'], precise=True)))\n",
    "references_zh = pd.Series(list(cont.expand_texts(scores_zh['reference'], precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for translations\n",
    "translation_cs = pd.Series(list(cont.expand_texts(scores_cs['translation'], precise=True)))\n",
    "translation_de = pd.Series(list(cont.expand_texts(scores_de['translation'], precise=True)))\n",
    "translation_ru = pd.Series(list(cont.expand_texts(scores_ru['translation'], precise=True)))\n",
    "translation_zh = pd.Series(list(cont.expand_texts(scores_zh['translation'], precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.    ---->  \n",
      " it is now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.\n"
     ]
    }
   ],
   "source": [
    "# Check how expanding contractions worked\n",
    "print(scores_de['translation'][6], '   ---->  \\n',translation_de[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# release references and 'garbage collect' to free memory\n",
    "import gc\n",
    "del cont\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(series, pontuation = False, zh = False):\n",
    "    processed_corpus = []\n",
    "    #stop_words = set(stopwords.words())\n",
    "    for i in tqdm(range(len(series))):\n",
    "        text = series[i]\n",
    "        \n",
    "        #remove tags\n",
    "        text = BeautifulSoup(text).get_text()\n",
    "        \n",
    "        #Remove punctuations\n",
    "        if pontuation==True:\n",
    "            text = re.sub(r'[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，？、~@#￥%……&*（）:：；《）《》“”()»〔〕-]+', ' ', text)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Convert to list from string\n",
    "        \n",
    "        # For en-zh language pair use jieba for words cut\n",
    "        if zh == True:\n",
    "            text = jieba.cut_for_search(text)\n",
    "        else:\n",
    "            text = text.split()\n",
    "\n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        #text = [lem.lemmatize(word) for word in text if not word in stop_words] \n",
    "        text = \" \".join(text)\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ab23553b084659af4d478c611b059e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11585.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6859983ce6e48ba851c260adec25a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21704.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0520221cff946d8912e368aa94c78a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8021c8673246405daaeacc17903ed70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\murmu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.712 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959e1da3635a42a9beb83c79f0f540b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17980.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0296693ae38340839adff299c5178c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references_cs = preprocessing(references_cs)\n",
    "references_de = preprocessing(references_de)\n",
    "references_en_fi = preprocessing(scores_en_fi['reference'])\n",
    "references_en_zh = preprocessing(scores_en_zh['reference'], zh = True)\n",
    "references_ru = preprocessing(references_ru)\n",
    "references_zh = preprocessing(references_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173a49a6891b4948b9b5d224f807dc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11585.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eec13e71b774894bbd9845ac93b1de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21704.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98259faa70f64c3ba26e9b819774a2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833eee134cb24f8ea12fcae02f7e2f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06dac46b470c4b74afc51baecb36de2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17980.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68e650a35eb46fcba03a2859546892a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translation_cs = preprocessing(translation_cs)\n",
    "translation_de = preprocessing(translation_de)\n",
    "translation_en_fi = preprocessing(scores_en_fi['translation'])\n",
    "translation_en_zh = preprocessing(scores_en_zh['translation'], zh = True)\n",
    "translation_ru = preprocessing(translation_ru)\n",
    "translation_zh = preprocessing(translation_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS from your file text_mining1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrf_metric(translation, references):\n",
    "    chrf_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = chrf.sentence_chrf([translation[i]], references[i])\n",
    "        chrf_metric.append(row)\n",
    "    return chrf_metric\n",
    "\n",
    "chf_cs = chrf_metric(translation_cs, references_cs)\n",
    "scores_cs['chf'] = chf_cs\n",
    "chf_de = chrf_metric(translation_de, references_de)\n",
    "scores_de['chf'] = chf_de\n",
    "chf_en_fi = chrf_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['chf'] = chf_en_fi\n",
    "chf_en_zh = chrf_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['chf'] = chf_en_zh\n",
    "chf_ru = chrf_metric(translation_ru, references_ru)\n",
    "scores_ru['chf'] = chf_ru\n",
    "chf_zh = chrf_metric(translation_zh, references_zh)\n",
    "scores_zh['chf'] = chf_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gleu_metric(translation, references):\n",
    "    gleu_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = gleu.sentence_gleu([str(references[i]).split()], str(translation[i]).split(), min_len=1, max_len=2)\n",
    "        gleu_metric.append(row)\n",
    "    return gleu_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_cs = gleu_metric(translation_cs, references_cs)\n",
    "scores_cs['gleu'] = gleu_cs\n",
    "\n",
    "gleu_de = gleu_metric(translation_de, references_de)\n",
    "scores_de['gleu'] = gleu_de\n",
    "\n",
    "gleu_en_fi = gleu_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['gleu'] = gleu_en_fi\n",
    "\n",
    "gleu_en_zh = gleu_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['gleu'] = gleu_en_zh\n",
    "\n",
    "gleu_ru = gleu_metric(translation_ru, references_ru)\n",
    "scores_ru['gleu'] = gleu_ru\n",
    "\n",
    "gleu_zh = gleu_metric(translation_zh, references_zh)\n",
    "scores_zh['gleu'] = gleu_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteor_metric(translation, references):\n",
    "    meteor_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (meteor.meteor_score([translation[i]], references[i]))\n",
    "        meteor_metric.append(row)\n",
    "    return meteor_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_cs = meteor_metric(translation_cs, references_cs)\n",
    "scores_cs['meteor'] = meteor_cs\n",
    "\n",
    "meteor_de = meteor_metric(translation_de, references_de)\n",
    "scores_de['meteor'] = meteor_de\n",
    "\n",
    "meteor_en_fi = meteor_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['meteor'] = meteor_en_fi\n",
    "\n",
    "meteor_en_zh = meteor_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['meteor'] = meteor_en_zh\n",
    "\n",
    "meteor_ru = meteor_metric(translation_ru, references_ru)\n",
    "scores_ru['meteor'] = meteor_ru\n",
    "\n",
    "meteor_zh = meteor_metric(translation_zh, references_zh)\n",
    "scores_zh['meteor'] = meteor_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_metric(translation, references):\n",
    "    bleu_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (bleu.sentence_bleu([translation[i]], references[i]))\n",
    "        bleu_metric.append(row)\n",
    "    return bleu_metric\n",
    "\n",
    "bleu_cs = bleu_metric(translation_cs, references_cs)\n",
    "scores_cs['bleu'] = bleu_cs\n",
    "\n",
    "bleu_de = bleu_metric(translation_de, references_de)\n",
    "scores_de['bleu'] = bleu_de\n",
    "\n",
    "bleu_en_fi = bleu_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['bleu'] = bleu_en_fi\n",
    "\n",
    "bleu_en_zh = bleu_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['bleu'] = bleu_en_zh\n",
    "\n",
    "bleu_ru = bleu_metric(translation_ru, references_ru)\n",
    "scores_ru['bleu'] = bleu_ru\n",
    "\n",
    "bleu_zh = bleu_metric(translation_zh, references_zh)\n",
    "scores_zh['bleu'] = bleu_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nist_metric(translation, references):\n",
    "    nist_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (nist.sentence_nist([translation[i]], references[i],n=1))\n",
    "        nist_metric.append(row)\n",
    "    return nist_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_cs = nist_metric(translation_cs, references_cs)\n",
    "scores_cs['nist'] = nist_cs\n",
    "\n",
    "nist_de = nist_metric(translation_de, references_de)\n",
    "scores_de['nist'] = nist_de\n",
    "\n",
    "nist_en_fi = nist_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['nist'] = nist_en_fi\n",
    "\n",
    "nist_en_zh = nist_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['nist'] = nist_en_zh\n",
    "\n",
    "nist_ru = nist_metric(translation_ru, references_ru)\n",
    "scores_ru['nist'] = nist_ru\n",
    "\n",
    "nist_zh = nist_metric(translation_zh, references_zh)\n",
    "scores_zh['nist'] = nist_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(scores):\n",
    "    chf = scores['z-score'].corr(scores['chf'])\n",
    "    gleu = scores['z-score'].corr(scores['gleu'])\n",
    "    meteor = scores['z-score'].corr(scores['meteor'])\n",
    "    bleu = scores['z-score'].corr(scores['bleu'])\n",
    "    nist = scores['z-score'].corr(scores['nist'])\n",
    "    #ribes = scores['z-score'].corr(scores['ribes'])\n",
    "    corr = [chf, gleu, meteor, bleu, nist]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cs = corr(scores_cs)\n",
    "corr_de = corr(scores_de)\n",
    "corr_en_fi = corr(scores_en_fi)\n",
    "corr_en_zh = corr(scores_en_zh)\n",
    "corr_ru = corr(scores_ru)\n",
    "corr_zh = corr(scores_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['chf', 'gleu', 'meteor', 'bleu', 'nist']\n",
    "data = []\n",
    "index = ['scores_cs', 'scores_de', 'scores_en_fi', 'scores_en_zh', 'scores_ru', 'scores_zh']\n",
    "correlation = pd.DataFrame(data, columns=columns, index = index)\n",
    "correlation.iloc[0] = corr_cs\n",
    "correlation.iloc[1] = corr_de\n",
    "correlation.iloc[2] = corr_en_fi\n",
    "correlation.iloc[3] = corr_en_zh\n",
    "correlation.iloc[4] = corr_ru\n",
    "correlation.iloc[5] = corr_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.46223</td>\n",
       "      <td>0.427899</td>\n",
       "      <td>0.439972</td>\n",
       "      <td>0.46878</td>\n",
       "      <td>0.334652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341142</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.308036</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_fi</th>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.494636</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>0.619928</td>\n",
       "      <td>0.423513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_zh</th>\n",
       "      <td>0.423398</td>\n",
       "      <td>0.449157</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.468428</td>\n",
       "      <td>0.432876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361416</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336737</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326399</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   chf      gleu    meteor      bleu      nist\n",
       "scores_cs      0.46223  0.427899  0.439972   0.46878  0.334652\n",
       "scores_de     0.341142  0.310098  0.308036  0.346757  0.227363\n",
       "scores_en_fi  0.611567  0.494636  0.491475  0.619928  0.423513\n",
       "scores_en_zh  0.423398  0.449157  0.453092  0.468428  0.432876\n",
       "scores_ru     0.361416  0.333536  0.336737  0.367581   0.26207\n",
       "scores_zh     0.341207  0.317924  0.326399  0.351904  0.244659"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline correlation\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cosine similarities for sentences using word embeddings\n",
    " - we use pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english stopwords\n",
    "english_stop_words = set(stopwords.words('english')).union(STOP_WORDS)\n",
    "\n",
    "# chinese stopwords\n",
    "stopwords_zh = codecs.open('stopwords-zh.txt', 'r', 'utf-8').read().split(',')\n",
    "\n",
    "# finnish stopwords\n",
    "stopwords_fi = codecs.open('stop_words_finnish.txt', 'r').read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords, lemmatize if needed\n",
    "\n",
    "def words_count(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "   \n",
    "    # delete stop words\n",
    "    tokens = list(filter(lambda x: x not in english_stop_words, tokens))\n",
    "    \n",
    "    # lemmatize the words WordNetLemmatizer from nlkt\n",
    "    # this made Pearson correlation lower - so was commented\n",
    "    \n",
    "    # tokens = list(map(lambda x: nltk.WordNetLemmatizer().lemmatize(x), tokens))\n",
    "    \n",
    "    # With spacy lemmatizer (worked better than nltk)\n",
    "    # it is commented because it made correlation lower\n",
    "    # tokens = list(map(lambda x: [token.lemma_ for token in nlp(x)], tokens))\n",
    "    # tokens = list(itertools.chain(*tokens))\n",
    "    \n",
    "    return list(Counter(tokens).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords, lemmatize if needed\n",
    "\n",
    "def words_count_zh(text):\n",
    "#     # Tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # delete stop words\n",
    "    tokens = list(filter(lambda x: x not in stopwords_zh, tokens))\n",
    "    return list(Counter(tokens).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords, lemmatize if needed\n",
    "def words_count_fi(text):\n",
    "#   # Tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # delete stop words\n",
    "    tokens = list(filter(lambda x: x not in stopwords_fi, tokens))\n",
    "\n",
    "    return list(Counter(tokens).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe (Global Vectors for Word Representation) word vectors.\n",
    "# For cs, de, ru, zh that translated to english\n",
    "# 6B tokens, 400K vocab, (300d version).\n",
    "# Download file from https://www.kaggle.com/thanakomsn/glove6b300dtxt\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "    return word_to_vec_map\n",
    "# Download glove.6B.300d.txt to a folder with this jupiter notebook\n",
    "word_to_vec_map = read_glove_vecs('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese word vector\n",
    "# Download word vectors file from https://www.kaggle.com/kerneler/starter-sgns-merge-word-18e5b7b5-9/execution?select=sgns.merge.word\n",
    "word_to_vec_zh = read_glove_vecs('sgns.merge.word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finnish word vectors\n",
    "# Download from https://github.com/jmyrberg/finnish-word-embeddings\n",
    "word_to_vec_fi = read_glove_vecs('cc.fi.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column for calculating cosine similarity\n",
    "scores_cs['Cosine_similarity'], scores_de['Cosine_similarity'] = 0, 0\n",
    "scores_ru['Cosine_similarity'], scores_zh['Cosine_similarity'] = 0, 0\n",
    "scores_en_fi['Cosine_similarity'], scores_en_zh['Cosine_similarity'] = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists with languages to work with\n",
    "ref_list = [references_cs, references_de, references_ru, references_zh, references_en_fi, references_en_zh]\n",
    "trans_list = [translation_cs, translation_de, translation_ru, translation_zh, translation_en_fi,  translation_en_zh]\n",
    "scores = [scores_cs, scores_de, scores_ru, scores_zh, scores_en_fi, scores_en_zh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c765fde515b4c68a0f528449ea6079e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11585.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ccf27fa23b43cb95a081deff129ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21704.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673850b170dc45df845e0657115d2a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17980.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecdac943019487e954912684de0ea65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fc144f08864861a53a03c3dd51db8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8329bd00c903464e9159aa84737fab33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check cosine similarity between two sentences (reference and translation)\n",
    "# Use different pre-trained word embeddings:\n",
    "# GloVe - for cs, de, ru, zh that translated to english\n",
    "# 'sgns.merge.word' - for en-zh pair\n",
    "# 'cc.fi.300.vec' - for en-fi pair\n",
    "\n",
    "for doc in range(len(scores)):\n",
    "    ref_embeddings = np.zeros((len(scores[doc]),300))\n",
    "    tr_embeddings = np.zeros((len(scores[doc]),300))\n",
    "    \n",
    "    norms_doc_embeddings_tr = np.zeros((len(trans_list[doc]),1))\n",
    "    norms_doc_embeddings_ref = np.zeros((len(ref_list[doc]),1))\n",
    "    \n",
    "    Cosine_similarity = []\n",
    "    \n",
    "    for i in tqdm(range(len(trans_list[doc]))):\n",
    "        if doc == 5:\n",
    "            words_freq = words_count_zh(trans_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_zh[\"a\"].shape)\n",
    "        elif doc == 4:\n",
    "            words_freq = words_count_fi(trans_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_fi[\"a\"].shape)\n",
    "        else:\n",
    "            words_freq = words_count(trans_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_map[\"a\"].shape)\n",
    "        num_words = 0\n",
    "        for word_freq in words_freq:\n",
    "            word = word_freq[0]\n",
    "            freq = word_freq[1]\n",
    "            try:\n",
    "                #adding word embeddings for each word in the document\n",
    "                if doc == 5:\n",
    "                    doc_embeddings += (word_to_vec_zh[word] * freq)\n",
    "                elif doc == 4:\n",
    "                    doc_embeddings += (word_to_vec_fi[word] * freq)\n",
    "                else:\n",
    "                    doc_embeddings += (word_to_vec_map[word] * freq)\n",
    "                num_words += freq\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            # doing average\n",
    "            doc_embeddings /= num_words\n",
    "        except:\n",
    "            print(\"divide by zero encountered for article at index \"+str(i))\n",
    "            continue\n",
    "        norms_doc_embeddings_tr[i,:] = np.sqrt(np.dot(doc_embeddings,doc_embeddings))\n",
    "        tr_embeddings[i,:] = doc_embeddings\n",
    "        \n",
    "        \n",
    "        if doc == 5:\n",
    "            words_freq = words_count_zh(ref_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_zh[\"a\"].shape)\n",
    "        elif doc == 4:\n",
    "            words_freq = words_count_fi(trans_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_fi[\"a\"].shape)\n",
    "\n",
    "        else:\n",
    "            words_freq = words_count(ref_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_map[\"a\"].shape)\n",
    "        num_words = 0\n",
    "        for word_freq in words_freq:\n",
    "            word = word_freq[0]\n",
    "            freq = word_freq[1]\n",
    "            try:\n",
    "                #adding word embeddings for each word in the document\n",
    "                if doc == 5:\n",
    "                    doc_embeddings += (word_to_vec_zh[word] * freq)\n",
    "                elif doc == 4:\n",
    "                    doc_embeddings += (word_to_vec_fi[word] * freq)\n",
    "                else:\n",
    "                    doc_embeddings += (word_to_vec_map[word] * freq)\n",
    "                num_words += freq\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            # doing average\n",
    "            doc_embeddings /= num_words\n",
    "        except:\n",
    "            print(\"divide by zero encountered for article at index \"+str(i))\n",
    "            continue\n",
    "        norms_doc_embeddings_ref[i,:] = np.sqrt(np.dot(doc_embeddings,doc_embeddings))\n",
    "        ref_embeddings[i,:] = doc_embeddings\n",
    "    \n",
    "        # Calculate cosine similarity using dot product of two vectors\n",
    "        # and norms of vectors\n",
    "        \n",
    "        #  The cosine similarity depends on the angle between  vectors .\n",
    "        #  If  vectors  are very similar, their cosine similarity will be close to 1\n",
    "        norm_prods = norms_doc_embeddings_ref[i] * norms_doc_embeddings_tr[i]\n",
    "        dot_prod = np.dot(ref_embeddings[i], tr_embeddings[i].reshape(300,1))\n",
    "        cos_similarity = dot_prod / norm_prods\n",
    "        \n",
    "        Cosine_similarity.append(float(cos_similarity))\n",
    "    \n",
    "    scores[doc]['Cosine_similarity'] = pd.Series(Cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_new(scores):\n",
    "    chf = scores['z-score'].corr(scores['chf'])\n",
    "    gleu = scores['z-score'].corr(scores['gleu'])\n",
    "    meteor = scores['z-score'].corr(scores['meteor'])\n",
    "    bleu = scores['z-score'].corr(scores['bleu'])\n",
    "    nist = scores['z-score'].corr(scores['nist'])\n",
    "    cos = scores['z-score'].corr(scores['Cosine_similarity'])\n",
    "    corr = [chf, gleu, meteor, bleu, nist, cos]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cs_new = corr_new(scores_cs)\n",
    "corr_de_new = corr_new(scores_de)\n",
    "corr_ru_new = corr_new(scores_ru)\n",
    "corr_zh_new = corr_new(scores_zh)\n",
    "corr_en_fi_new = corr_new(scores_en_fi)\n",
    "corr_en_zh_new = corr_new(scores_en_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['chf', 'gleu', 'meteor', 'bleu', 'nist', 'Cos_sim']\n",
    "data = []\n",
    "index = ['scores_cs', 'scores_de', 'scores_en_fi', 'scores_en_zh', 'scores_ru', 'scores_zh']\n",
    "correlation = pd.DataFrame(data, columns=columns, index = index)\n",
    "correlation.iloc[0] = corr_cs_new\n",
    "correlation.iloc[1] = corr_de_new\n",
    "\n",
    "\n",
    "correlation.iloc[2] = corr_en_fi_new\n",
    "correlation.iloc[3] = corr_en_zh_new\n",
    "\n",
    "correlation.iloc[4] = corr_ru_new\n",
    "correlation.iloc[5] = corr_zh_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "      <th>Cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.46223</td>\n",
       "      <td>0.427899</td>\n",
       "      <td>0.439972</td>\n",
       "      <td>0.46878</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.368188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341142</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.308036</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "      <td>0.290548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_fi</th>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.494636</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>0.619928</td>\n",
       "      <td>0.423513</td>\n",
       "      <td>-0.00617554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_zh</th>\n",
       "      <td>0.423398</td>\n",
       "      <td>0.449157</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.468428</td>\n",
       "      <td>0.432876</td>\n",
       "      <td>0.360268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361416</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336737</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "      <td>0.304984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326399</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "      <td>0.27196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   chf      gleu    meteor      bleu      nist     Cos_sim\n",
       "scores_cs      0.46223  0.427899  0.439972   0.46878  0.334652    0.368188\n",
       "scores_de     0.341142  0.310098  0.308036  0.346757  0.227363    0.290548\n",
       "scores_en_fi  0.611567  0.494636  0.491475  0.619928  0.423513 -0.00617554\n",
       "scores_en_zh  0.423398  0.449157  0.453092  0.468428  0.432876    0.360268\n",
       "scores_ru     0.361416  0.333536  0.336737  0.367581   0.26207    0.304984\n",
       "scores_zh     0.341207  0.317924  0.326399  0.351904  0.244659     0.27196"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate correlation between z-score and cosine_similarity columns\n",
    "# as well\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libvoikko import Voikko\n",
    "Voikko.setLibrarySearchPath(\"c:\\Voikko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "VoikkoException",
     "evalue": "Initialization of Voikko failed: No valid dictionaries were found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mVoikkoException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-de461feb60fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Define a Voikko class for Finnish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibvoikko\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVoikko\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"fi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#A word that might or might not be in base form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\libvoikko.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, language, path)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mVoikkoException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initialization of Voikko failed: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0municode_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"UTF-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mVoikkoException\u001b[0m: Initialization of Voikko failed: No valid dictionaries were found"
     ]
    }
   ],
   "source": [
    "#Import the Voikko library\n",
    "import libvoikko\n",
    "\n",
    "#Define a Voikko class for Finnish\n",
    "v = libvoikko.Voikko(u\"fi\")\n",
    "\n",
    "#A word that might or might not be in base form\n",
    "#Finnish word \"kissoja\" means \"cats\" in English\n",
    "word = \"kissoja\"\n",
    "\n",
    "#Analyze the word\n",
    "voikko_dict = v.analyze(word)\n",
    "\n",
    "#Extract the base form as\n",
    "#analyze() function returns various info for the word\n",
    "word_baseform = voikko_dict[0]['BASEFORM']\n",
    "\n",
    "#Print the base form of the word\n",
    "#This should print \"kissa\", which is \"cat\" in English\n",
    "print(word_baseform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-1d82dd35e833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    from urllib import urlretrieve\n",
    "else:\n",
    "    from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'uralicNLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-a182ba28163c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0muralicNLP\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0muralicApi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0muralicApi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupported_languages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'uralicNLP'"
     ]
    }
   ],
   "source": [
    "from uralicNLP import uralicApi\n",
    "uralicApi.supported_languages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
