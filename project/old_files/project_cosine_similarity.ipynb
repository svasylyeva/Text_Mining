{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Import libraries\n",
    "import nltk.translate.chrf_score as chrf\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "import nltk.translate.gleu_score as gleu\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "try:\n",
    "  nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "  nltk.download('punkt')\n",
    "import nltk.translate.meteor_score as meteor\n",
    "import nltk.translate.nist_score as nist\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "#import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import itertools\n",
    "\n",
    "import codecs\n",
    "import jieba\n",
    "\n",
    "#Import nltk Snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cs = pd.read_csv(\"corpus/cs-en/scores.csv\")\n",
    "scores_de = pd.read_csv(\"corpus/de-en/scores.csv\")\n",
    "scores_en_fi = pd.read_csv(\"corpus/en-fi/scores.csv\")\n",
    "scores_en_zh = pd.read_csv(\"corpus/en-zh/scores.csv\")\n",
    "scores_ru = pd.read_csv(\"corpus/ru-en/scores.csv\")\n",
    "scores_zh = pd.read_csv(\"corpus/zh-en/scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with contractions using pycontractes lib that chooses a proper form\n",
    "# instead ambiguous \"you'd\" -> \"you had / you would\"\n",
    "\n",
    "from pycontractions import Contractions\n",
    "\n",
    "# Download 'GoogleNews-vectors-negative300.bin' file to a folder with this jupiter notebook\n",
    "# if needed (size is more 3 Gb)\n",
    "# archive on - https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "cont = Contractions('GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# prevents loading on first expand_texts call\n",
    "# usually takes time\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = scores_de.translation\n",
    "a[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.Series(list(cont.expand_texts(a, precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how contracion expanding worked\n",
    "b[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As contraction extractions work really slow\n",
    "# they were done outside preprocessing function\n",
    "references_cs = pd.Series(list(cont.expand_texts(scores_cs['reference'], precise=True)))\n",
    "references_de = pd.Series(list(cont.expand_texts(scores_de['reference'], precise=True)))\n",
    "references_ru = pd.Series(list(cont.expand_texts(scores_ru['reference'], precise=True)))\n",
    "references_zh = pd.Series(list(cont.expand_texts(scores_zh['reference'], precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for translations\n",
    "translation_cs = pd.Series(list(cont.expand_texts(scores_cs['translation'], precise=True)))\n",
    "translation_de = pd.Series(list(cont.expand_texts(scores_de['translation'], precise=True)))\n",
    "translation_ru = pd.Series(list(cont.expand_texts(scores_ru['translation'], precise=True)))\n",
    "translation_zh = pd.Series(list(cont.expand_texts(scores_zh['translation'], precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.    ---->  \n",
      " it is now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.\n"
     ]
    }
   ],
   "source": [
    "# Check how expanding contractions worked\n",
    "print(scores_de['translation'][6], '   ---->  \\n',translation_de[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# release references and 'garbage collect' to free memory\n",
    "import gc\n",
    "del cont\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(series, pontuation = False, zh = False):\n",
    "    processed_corpus = []\n",
    "    #stop_words = set(stopwords.words())\n",
    "    for i in tqdm(range(len(series))):\n",
    "        text = series[i]\n",
    "        \n",
    "        #remove tags\n",
    "        text = BeautifulSoup(text).get_text()\n",
    "        \n",
    "        #Remove punctuations\n",
    "        if pontuation==True:\n",
    "            text = re.sub(r'[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，？、~@#￥%……&*（）:：；《）《》“”()»〔〕-]+', ' ', text)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Convert to list from string\n",
    "        \n",
    "        # For en-zh language pair use jieba for words cut\n",
    "        if zh == True:\n",
    "            text = jieba.cut_for_search(text)\n",
    "        else:\n",
    "            text = text.split()\n",
    "\n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        #text = [lem.lemmatize(word) for word in text if not word in stop_words] \n",
    "        text = \" \".join(text)\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e64a045984e4bc0a82a983320a82815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11585.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad9ce33a7bc4fd993a6913b47009e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21704.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ef6ac0e1304534993200ebcecaf5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cb5a6de1424635b7aff46e9bb9446d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\murmu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.839 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bf0be389f54392b11a79a1c22a3be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17980.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4feaf46eb974c3fbafdcfe3924f1f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references_cs = preprocessing(references_cs)\n",
    "references_de = preprocessing(references_de)\n",
    "references_en_fi = preprocessing(scores_en_fi['reference'])\n",
    "references_en_zh = preprocessing(scores_en_zh['reference'], zh = True)\n",
    "references_ru = preprocessing(references_ru)\n",
    "references_zh = preprocessing(references_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2871679b0b1242518906e583e41b2ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11585.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863122cd41e746b1aef12c8899b6e091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21704.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fd52e488334127aa2174880889cb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373dcb55562344fa9a15b64ddf6670bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66106a3f1d3f44acba5c67c3d219629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17980.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dc4ad6ae9d40df9e54bd1b7d2e734c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translation_cs = preprocessing(translation_cs)\n",
    "translation_de = preprocessing(translation_de)\n",
    "translation_en_fi = preprocessing(scores_en_fi['translation'])\n",
    "translation_en_zh = preprocessing(scores_en_zh['translation'], zh = True)\n",
    "translation_ru = preprocessing(translation_ru)\n",
    "translation_zh = preprocessing(translation_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS from your file text_mining1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrf_metric(translation, references):\n",
    "    chrf_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = chrf.sentence_chrf([translation[i]], references[i])\n",
    "        chrf_metric.append(row)\n",
    "    return chrf_metric\n",
    "\n",
    "chf_cs = chrf_metric(translation_cs, references_cs)\n",
    "scores_cs['chf'] = chf_cs\n",
    "chf_de = chrf_metric(translation_de, references_de)\n",
    "scores_de['chf'] = chf_de\n",
    "chf_en_fi = chrf_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['chf'] = chf_en_fi\n",
    "chf_en_zh = chrf_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['chf'] = chf_en_zh\n",
    "chf_ru = chrf_metric(translation_ru, references_ru)\n",
    "scores_ru['chf'] = chf_ru\n",
    "chf_zh = chrf_metric(translation_zh, references_zh)\n",
    "scores_zh['chf'] = chf_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gleu_metric(translation, references):\n",
    "    gleu_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = gleu.sentence_gleu([str(references[i]).split()], str(translation[i]).split(), min_len=1, max_len=2)\n",
    "        gleu_metric.append(row)\n",
    "    return gleu_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_cs = gleu_metric(translation_cs, references_cs)\n",
    "scores_cs['gleu'] = gleu_cs\n",
    "\n",
    "gleu_de = gleu_metric(translation_de, references_de)\n",
    "scores_de['gleu'] = gleu_de\n",
    "\n",
    "gleu_en_fi = gleu_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['gleu'] = gleu_en_fi\n",
    "\n",
    "gleu_en_zh = gleu_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['gleu'] = gleu_en_zh\n",
    "\n",
    "gleu_ru = gleu_metric(translation_ru, references_ru)\n",
    "scores_ru['gleu'] = gleu_ru\n",
    "\n",
    "gleu_zh = gleu_metric(translation_zh, references_zh)\n",
    "scores_zh['gleu'] = gleu_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteor_metric(translation, references):\n",
    "    meteor_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (meteor.meteor_score([translation[i]], references[i]))\n",
    "        meteor_metric.append(row)\n",
    "    return meteor_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_cs = meteor_metric(translation_cs, references_cs)\n",
    "scores_cs['meteor'] = meteor_cs\n",
    "\n",
    "meteor_de = meteor_metric(translation_de, references_de)\n",
    "scores_de['meteor'] = meteor_de\n",
    "\n",
    "meteor_en_fi = meteor_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['meteor'] = meteor_en_fi\n",
    "\n",
    "meteor_en_zh = meteor_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['meteor'] = meteor_en_zh\n",
    "\n",
    "meteor_ru = meteor_metric(translation_ru, references_ru)\n",
    "scores_ru['meteor'] = meteor_ru\n",
    "\n",
    "meteor_zh = meteor_metric(translation_zh, references_zh)\n",
    "scores_zh['meteor'] = meteor_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_metric(translation, references):\n",
    "    bleu_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (bleu.sentence_bleu([translation[i]], references[i]))\n",
    "        bleu_metric.append(row)\n",
    "    return bleu_metric\n",
    "\n",
    "bleu_cs = bleu_metric(translation_cs, references_cs)\n",
    "scores_cs['bleu'] = bleu_cs\n",
    "\n",
    "bleu_de = bleu_metric(translation_de, references_de)\n",
    "scores_de['bleu'] = bleu_de\n",
    "\n",
    "bleu_en_fi = bleu_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['bleu'] = bleu_en_fi\n",
    "\n",
    "bleu_en_zh = bleu_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['bleu'] = bleu_en_zh\n",
    "\n",
    "bleu_ru = bleu_metric(translation_ru, references_ru)\n",
    "scores_ru['bleu'] = bleu_ru\n",
    "\n",
    "bleu_zh = bleu_metric(translation_zh, references_zh)\n",
    "scores_zh['bleu'] = bleu_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nist_metric(translation, references):\n",
    "    nist_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (nist.sentence_nist([translation[i]], references[i],n=1))\n",
    "        nist_metric.append(row)\n",
    "    return nist_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_cs = nist_metric(translation_cs, references_cs)\n",
    "scores_cs['nist'] = nist_cs\n",
    "\n",
    "nist_de = nist_metric(translation_de, references_de)\n",
    "scores_de['nist'] = nist_de\n",
    "\n",
    "nist_en_fi = nist_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['nist'] = nist_en_fi\n",
    "\n",
    "nist_en_zh = nist_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['nist'] = nist_en_zh\n",
    "\n",
    "nist_ru = nist_metric(translation_ru, references_ru)\n",
    "scores_ru['nist'] = nist_ru\n",
    "\n",
    "nist_zh = nist_metric(translation_zh, references_zh)\n",
    "scores_zh['nist'] = nist_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(scores):\n",
    "    chf = scores['z-score'].corr(scores['chf'])\n",
    "    gleu = scores['z-score'].corr(scores['gleu'])\n",
    "    meteor = scores['z-score'].corr(scores['meteor'])\n",
    "    bleu = scores['z-score'].corr(scores['bleu'])\n",
    "    nist = scores['z-score'].corr(scores['nist'])\n",
    "    #ribes = scores['z-score'].corr(scores['ribes'])\n",
    "    corr = [chf, gleu, meteor, bleu, nist]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cs = corr(scores_cs)\n",
    "corr_de = corr(scores_de)\n",
    "corr_en_fi = corr(scores_en_fi)\n",
    "corr_en_zh = corr(scores_en_zh)\n",
    "corr_ru = corr(scores_ru)\n",
    "corr_zh = corr(scores_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['chf', 'gleu', 'meteor', 'bleu', 'nist']\n",
    "data = []\n",
    "index = ['scores_cs', 'scores_de', 'scores_en_fi', 'scores_en_zh', 'scores_ru', 'scores_zh']\n",
    "correlation = pd.DataFrame(data, columns=columns, index = index)\n",
    "correlation.iloc[0] = corr_cs\n",
    "correlation.iloc[1] = corr_de\n",
    "correlation.iloc[2] = corr_en_fi\n",
    "correlation.iloc[3] = corr_en_zh\n",
    "correlation.iloc[4] = corr_ru\n",
    "correlation.iloc[5] = corr_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.46223</td>\n",
       "      <td>0.427899</td>\n",
       "      <td>0.439964</td>\n",
       "      <td>0.46878</td>\n",
       "      <td>0.334652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.30805</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_fi</th>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.494636</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>0.619928</td>\n",
       "      <td>0.423513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_zh</th>\n",
       "      <td>0.423398</td>\n",
       "      <td>0.449157</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.468428</td>\n",
       "      <td>0.432876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361418</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336746</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326359</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   chf      gleu    meteor      bleu      nist\n",
       "scores_cs      0.46223  0.427899  0.439964   0.46878  0.334652\n",
       "scores_de     0.341149  0.310098   0.30805  0.346757  0.227363\n",
       "scores_en_fi  0.611567  0.494636  0.491475  0.619928  0.423513\n",
       "scores_en_zh  0.423398  0.449157  0.453092  0.468428  0.432876\n",
       "scores_ru     0.361418  0.333536  0.336746  0.367581   0.26207\n",
       "scores_zh     0.341207  0.317924  0.326359  0.351904  0.244659"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline correlation\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cosine similarities for sentences using word embeddings\n",
    " - we use pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english stopwords\n",
    "english_stop_words = set(stopwords.words('english')).union(STOP_WORDS)\n",
    "\n",
    "# chinese stopwords\n",
    "stopwords_zh = codecs.open('stopwords-zh.txt', 'r', 'utf-8').read().split(',')\n",
    "\n",
    "# finnish stopwords\n",
    "stop_words_fi = get_stop_words('finnish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords, lemmatize if needed\n",
    "\n",
    "def words_count(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "   \n",
    "    # delete stop words\n",
    "    tokens = list(filter(lambda x: x not in english_stop_words, tokens))\n",
    "    \n",
    "    # lemmatize the words WordNetLemmatizer from nlkt\n",
    "    # this made Pearson correlation lower - so was commented\n",
    "    \n",
    "    # tokens = list(map(lambda x: nltk.WordNetLemmatizer().lemmatize(x), tokens))\n",
    "    \n",
    "    # With spacy lemmatizer (worked better than nltk)\n",
    "    # it is commented because it made correlation lower\n",
    "    # tokens = list(map(lambda x: [token.lemma_ for token in nlp(x)], tokens))\n",
    "    # tokens = list(itertools.chain(*tokens))\n",
    "    \n",
    "    return list(Counter(tokens).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords, lemmatize if needed\n",
    "\n",
    "def words_count_zh(text):\n",
    "#     # Tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # delete stop words\n",
    "    tokens = list(filter(lambda x: x not in stopwords_zh, tokens))\n",
    "    return list(Counter(tokens).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe (Global Vectors for Word Representation) word vectors.\n",
    "# For cs, de, ru, zh that translated to english\n",
    "# 6B tokens, 400K vocab, (300d version).\n",
    "# Download file from https://www.kaggle.com/thanakomsn/glove6b300dtxt\n",
    "# to a folder with this jupiter notebook (size 1Gb)\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "    return word_to_vec_map\n",
    "# Download glove.6B.300d.txt to a folder with this jupiter notebook\n",
    "word_to_vec_map = read_glove_vecs('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese word vector\n",
    "# Download word vectors file from https://www.kaggle.com/kerneler/starter-sgns-merge-word-18e5b7b5-9/execution?select=sgns.merge.word\n",
    "# file to a folder with this jupiter notebook (size 3.5Gb)\n",
    "word_to_vec_zh = read_glove_vecs('sgns.merge.word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could not find glove nodel for finnish,\n",
    "# so will work with finnish language separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column for calculating cosine similarity\n",
    "scores_cs['Cosine_similarity'], scores_de['Cosine_similarity'] = 0, 0\n",
    "scores_ru['Cosine_similarity'], scores_zh['Cosine_similarity'] = 0, 0\n",
    "scores_en_fi['Cosine_similarity'], scores_en_zh['Cosine_similarity'] = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists with languages to work with\n",
    "ref_list = [references_cs, references_de, references_ru, references_zh, references_en_zh]\n",
    "trans_list = [translation_cs, translation_de, translation_ru, translation_zh, translation_en_zh]\n",
    "scores = [scores_cs, scores_de, scores_ru, scores_zh, scores_en_zh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0abc0a618f84af1bb08f21c309435aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11585.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217b7afe7321455c9ae3f93f701d0eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21704.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2719ff090644453bde8464c3fa26543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17980.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9616720b3c44978b843fd6b14e9328a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd806406521402db0c75c7244e54318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check cosine similarity between two sentences (reference and translation)\n",
    "# Use different pre-trained word embeddings:\n",
    "# GloVe - for cs, de, ru, zh that translated to english\n",
    "# 'sgns.merge.word' - for en-zh pair\n",
    "\n",
    "for doc in range(len(scores)):\n",
    "    ref_embeddings = np.zeros((len(scores[doc]),300))\n",
    "    tr_embeddings = np.zeros((len(scores[doc]),300))\n",
    "    \n",
    "    norms_doc_embeddings_tr = np.zeros((len(trans_list[doc]),1))\n",
    "    norms_doc_embeddings_ref = np.zeros((len(ref_list[doc]),1))\n",
    "    \n",
    "    Cosine_similarity = []\n",
    "    \n",
    "    for i in tqdm(range(len(trans_list[doc]))):\n",
    "        if doc == 4:\n",
    "            words_freq = words_count_zh(trans_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_zh[\"a\"].shape)\n",
    "        \n",
    "        else:\n",
    "            words_freq = words_count(trans_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_map[\"a\"].shape)\n",
    "        num_words = 0\n",
    "        for word_freq in words_freq:\n",
    "            word = word_freq[0]\n",
    "            freq = word_freq[1]\n",
    "            try:\n",
    "                #adding word embeddings for each word in the document\n",
    "                if doc == 4:\n",
    "                    doc_embeddings += (word_to_vec_zh[word] * freq)\n",
    "                else:\n",
    "                    doc_embeddings += (word_to_vec_map[word] * freq)\n",
    "                num_words += freq\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            # doing average\n",
    "            doc_embeddings /= num_words\n",
    "        except:\n",
    "            print(\"divide by zero encountered for article at index \"+str(i))\n",
    "            continue\n",
    "        norms_doc_embeddings_tr[i,:] = np.sqrt(np.dot(doc_embeddings,doc_embeddings))\n",
    "        tr_embeddings[i,:] = doc_embeddings\n",
    "        \n",
    "        \n",
    "        if doc == 4:\n",
    "            words_freq = words_count_zh(ref_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_zh[\"a\"].shape)\n",
    "        else:\n",
    "            words_freq = words_count(ref_list[doc][i])\n",
    "            doc_embeddings = np.zeros(word_to_vec_map[\"a\"].shape)\n",
    "        num_words = 0\n",
    "        for word_freq in words_freq:\n",
    "            word = word_freq[0]\n",
    "            freq = word_freq[1]\n",
    "            try:\n",
    "                #adding word embeddings for each word in the document\n",
    "                if doc == 4:\n",
    "                    doc_embeddings += (word_to_vec_zh[word] * freq)\n",
    "                \n",
    "                else:\n",
    "                    doc_embeddings += (word_to_vec_map[word] * freq)\n",
    "                num_words += freq\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            # doing average\n",
    "            doc_embeddings /= num_words\n",
    "        except:\n",
    "            print(\"divide by zero encountered for article at index \"+str(i))\n",
    "            continue\n",
    "        norms_doc_embeddings_ref[i,:] = np.sqrt(np.dot(doc_embeddings,doc_embeddings))\n",
    "        ref_embeddings[i,:] = doc_embeddings\n",
    "    \n",
    "        # Calculate cosine similarity using dot product of two vectors\n",
    "        # and norms of vectors\n",
    "        \n",
    "        #  The cosine similarity depends on the angle between  vectors .\n",
    "        #  If  vectors  are very similar, their cosine similarity will be close to 1\n",
    "        norm_prods = norms_doc_embeddings_ref[i] * norms_doc_embeddings_tr[i]\n",
    "        dot_prod = np.dot(ref_embeddings[i], tr_embeddings[i].reshape(300,1))\n",
    "        cos_similarity = dot_prod / norm_prods\n",
    "        \n",
    "        Cosine_similarity.append(float(cos_similarity))\n",
    "    \n",
    "    scores[doc]['Cosine_similarity'] = pd.Series(Cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_new(scores):\n",
    "    chf = scores['z-score'].corr(scores['chf'])\n",
    "    gleu = scores['z-score'].corr(scores['gleu'])\n",
    "    meteor = scores['z-score'].corr(scores['meteor'])\n",
    "    bleu = scores['z-score'].corr(scores['bleu'])\n",
    "    nist = scores['z-score'].corr(scores['nist'])\n",
    "    cos = scores['z-score'].corr(scores['Cosine_similarity'])\n",
    "    corr = [chf, gleu, meteor, bleu, nist, cos]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cs_new = corr_new(scores_cs)\n",
    "corr_de_new = corr_new(scores_de)\n",
    "corr_ru_new = corr_new(scores_ru)\n",
    "corr_zh_new = corr_new(scores_zh)\n",
    "\n",
    "corr_en_zh_new = corr_new(scores_en_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['chf', 'gleu', 'meteor', 'bleu', 'nist', 'Cos_sim']\n",
    "data = []\n",
    "index = ['scores_cs', 'scores_de',  'scores_en_zh', 'scores_ru', 'scores_zh']\n",
    "correlation = pd.DataFrame(data, columns=columns, index = index)\n",
    "correlation.iloc[0] = corr_cs_new\n",
    "correlation.iloc[1] = corr_de_new\n",
    "correlation.iloc[2] = corr_en_zh_new\n",
    "\n",
    "correlation.iloc[3] = corr_ru_new\n",
    "correlation.iloc[4] = corr_zh_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "      <th>Cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.46223</td>\n",
       "      <td>0.427899</td>\n",
       "      <td>0.439964</td>\n",
       "      <td>0.46878</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.368188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.30805</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "      <td>0.290548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_zh</th>\n",
       "      <td>0.423398</td>\n",
       "      <td>0.449157</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.468428</td>\n",
       "      <td>0.432876</td>\n",
       "      <td>0.360268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361418</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336746</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "      <td>0.304984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326359</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "      <td>0.27196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   chf      gleu    meteor      bleu      nist   Cos_sim\n",
       "scores_cs      0.46223  0.427899  0.439964   0.46878  0.334652  0.368188\n",
       "scores_de     0.341149  0.310098   0.30805  0.346757  0.227363  0.290548\n",
       "scores_en_zh  0.423398  0.449157  0.453092  0.468428  0.432876  0.360268\n",
       "scores_ru     0.361418  0.333536  0.336746  0.367581   0.26207  0.304984\n",
       "scores_zh     0.341207  0.317924  0.326359  0.351904  0.244659   0.27196"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate correlation between z-score and cosine_similarity columns\n",
    "# for all language pairs except finnish\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fiinish language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764fbf1c9bdd4940b24e6e0f76a0d435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=995526.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285f2d5548d74be8b6f26b2637f7869c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bacea518eef46608dd9bb0c9d6145f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1961828.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ecb4bd3f904e8a81d27ef3126c3bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa83a161adbd4afd85eb1d9784b787b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1083389348.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained multilingual BERT model \n",
    "# to get sentence embeddings\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Long text about stars.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001192092896"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_a = tf.nn.l2_normalize(cls_embedding,0)        \n",
    "normalize_b = tf.nn.l2_normalize(cls_embedding,0)\n",
    "cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
    "float(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ec305cad5243e7aef5308d8589c4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Attention: this cell takes more than hour to calculate cosine similaritites\n",
    "\n",
    "# In order to compare the embeddings of sentences with  pretrained BERT \n",
    "# we use the value of the CLS token that corresponds to the first token of the output\n",
    "scores_en_fi['Cosine_similarity'] = 0\n",
    "Cosine = []\n",
    "for doc in tqdm(range(len(scores_en_fi))):\n",
    "    \n",
    "    # text from references_en_fi and translation_en_fi\n",
    "    text1 = references_en_fi[doc]\n",
    "    text2 = translation_en_fi[doc]\n",
    "    \n",
    "    encoded_input1 = tokenizer(text1, return_tensors='tf')\n",
    "    output1 = model(encoded_input1)\n",
    "    \n",
    "    encoded_input2 = tokenizer(text2, return_tensors='tf')\n",
    "    output2 = model(encoded_input2)\n",
    "    \n",
    "    last_hidden_states_1 = output1[0]\n",
    "    cls_embedding_1 = last_hidden_states_1[0][0]   \n",
    "    last_hidden_states_2 = output2[0]\n",
    "    cls_embedding_2 = last_hidden_states_2[0][0]\n",
    "    normalize_a = tf.nn.l2_normalize(cls_embedding_1,0)        \n",
    "    normalize_b = tf.nn.l2_normalize(cls_embedding_2,0)\n",
    "    cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
    "    Cosine.append(float(cos_similarity))\n",
    "    \n",
    "scores_en_fi['Cosine_similarity'] = pd.Series(Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['chf', 'gleu', 'meteor', 'bleu', 'nist', 'Cos_sim']\n",
    "data = []\n",
    "index = ['scores_cs', 'scores_de', 'scores_en_fi', 'scores_en_zh', 'scores_ru', 'scores_zh']\n",
    "correlation = pd.DataFrame(data, columns=columns, index = index)\n",
    "\n",
    "# Recalculate only en-fi score\n",
    "corr_en_fi_new = corr_new(scores_en_fi)\n",
    "\n",
    "correlation.iloc[0] = corr_cs_new\n",
    "correlation.iloc[1] = corr_de_new\n",
    "\n",
    "\n",
    "correlation.iloc[2] = corr_en_fi_new\n",
    "correlation.iloc[3] = corr_en_zh_new\n",
    "\n",
    "correlation.iloc[4] = corr_ru_new\n",
    "correlation.iloc[5] = corr_zh_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "      <th>Cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.46223</td>\n",
       "      <td>0.427899</td>\n",
       "      <td>0.439964</td>\n",
       "      <td>0.46878</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.368188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.30805</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "      <td>0.290548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_fi</th>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.494636</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>0.619928</td>\n",
       "      <td>0.423513</td>\n",
       "      <td>0.150627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_zh</th>\n",
       "      <td>0.423398</td>\n",
       "      <td>0.449157</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.468428</td>\n",
       "      <td>0.432876</td>\n",
       "      <td>0.360268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361418</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336746</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "      <td>0.304984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326359</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "      <td>0.27196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   chf      gleu    meteor      bleu      nist   Cos_sim\n",
       "scores_cs      0.46223  0.427899  0.439964   0.46878  0.334652  0.368188\n",
       "scores_de     0.341149  0.310098   0.30805  0.346757  0.227363  0.290548\n",
       "scores_en_fi  0.611567  0.494636  0.491475  0.619928  0.423513  0.150627\n",
       "scores_en_zh  0.423398  0.449157  0.453092  0.468428  0.432876  0.360268\n",
       "scores_ru     0.361418  0.333536  0.336746  0.367581   0.26207  0.304984\n",
       "scores_zh     0.341207  0.317924  0.326359  0.351904  0.244659   0.27196"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check our Pearson correlation coefficients\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will pre-process finish docs, trying to increase our correlation with z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention: this library can have some isuues during installation\n",
    "# Some advices how to resolve them from offical page - https://voikko.puimula.org/python.html\n",
    "\n",
    "#Import the Voikko to work with finnish lemmas\n",
    "import libvoikko\n",
    "\n",
    "# Check how this library works:\n",
    "\n",
    "#Define a Voikko class for Finnish\n",
    "v = libvoikko.Voikko(u\"fi\")\n",
    "\n",
    "#A word that might or might not be in base form\n",
    "word = \"lehmähauteen\"\n",
    "\n",
    "#Analyze the word\n",
    "voikko_dict = v.analyze(word)\n",
    "\n",
    "#Extract the base form as\n",
    "#analyze() function returns various info for the word\n",
    "word_baseform = voikko_dict[0]['BASEFORM']\n",
    "\n",
    "#Print the base form of the word\n",
    "print(word_baseform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords, lemmatize if needed\n",
    "def preprocess_fi(alltext):\n",
    "    processed_corpus = []\n",
    "    #stop_words = set(stopwords.words())\n",
    "    for i in tqdm(range(len(alltext))):\n",
    "        text = alltext[i]\n",
    "        # Tokenize\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        # tokenize text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        temp = []\n",
    "        #Define a Voikko class for Finnish\n",
    "        v = libvoikko.Voikko(u\"fi\")\n",
    "\n",
    "        # Change words to lemmas\n",
    "        for word in tokens:\n",
    "            voikko_dict = v.analyze(word)\n",
    "            try:\n",
    "                t = voikko_dict[0]['BASEFORM']\n",
    "                temp.append(t)\n",
    "            except:\n",
    "                temp.append(word)\n",
    "                continue\n",
    "        tokens = temp\n",
    "        # delete stop words\n",
    "        tokens = list(filter(lambda x: x not in stop_words_fi, tokens))\n",
    "        #Convert to lowercase because Voikko returns some words that begin from upper case\n",
    "        text = text.lower()\n",
    "        text = \" \".join(tokens)\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process finnish sentences (references and translations using preprocess_fi)\n",
    "references_en_fi_new = preprocess_fi(references_en_fi)\n",
    "translation_en_fi_new = preprocess_fi(translation_en_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e656691db475475ba7f33968a6194750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Attention: this cell takes more than hour to calculate cosine similaritites\n",
    "\n",
    "# In order to compare the embeddings of sentences with  pretrained BERT \n",
    "# we use the value of the CLS token that corresponds to the first token of the output\n",
    "scores_en_fi['Cosine_similarity'] = 0\n",
    "Cosine = []\n",
    "for doc in tqdm(range(len(scores_en_fi))):\n",
    "    \n",
    "    # text from references_en_fi and translation_en_fi\n",
    "    text1 = references_en_fi_new[doc]\n",
    "    text2 = translation_en_fi_new[doc]\n",
    "    \n",
    "    encoded_input1 = tokenizer(text1, return_tensors='tf')\n",
    "    output1 = model(encoded_input1)\n",
    "    \n",
    "    encoded_input2 = tokenizer(text2, return_tensors='tf')\n",
    "    output2 = model(encoded_input2)\n",
    "    \n",
    "    last_hidden_states_1 = output1[0]\n",
    "    cls_embedding_1 = last_hidden_states_1[0][0]   \n",
    "    last_hidden_states_2 = output2[0]\n",
    "    cls_embedding_2 = last_hidden_states_2[0][0]\n",
    "    normalize_a = tf.nn.l2_normalize(cls_embedding_1,0)        \n",
    "    normalize_b = tf.nn.l2_normalize(cls_embedding_2,0)\n",
    "    cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
    "    Cosine.append(float(cos_similarity))\n",
    "    \n",
    "scores_en_fi['Cosine_similarity'] = pd.Series(Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['chf', 'gleu', 'meteor', 'bleu', 'nist', 'Cos_sim']\n",
    "data = []\n",
    "index = ['scores_cs', 'scores_de', 'scores_en_fi', 'scores_en_zh', 'scores_ru', 'scores_zh']\n",
    "correlation = pd.DataFrame(data, columns=columns, index = index)\n",
    "# Recalculate only en-fi score\n",
    "corr_en_fi_new = corr_new(scores_en_fi)\n",
    "\n",
    "correlation.iloc[0] = corr_cs_new\n",
    "correlation.iloc[1] = corr_de_new\n",
    "\n",
    "correlation.iloc[2] = corr_en_fi_new\n",
    "correlation.iloc[3] = corr_en_zh_new\n",
    "\n",
    "correlation.iloc[4] = corr_ru_new\n",
    "correlation.iloc[5] = corr_zh_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "      <th>Cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.46223</td>\n",
       "      <td>0.427899</td>\n",
       "      <td>0.439964</td>\n",
       "      <td>0.46878</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.368188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.30805</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "      <td>0.290548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_fi</th>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.494636</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>0.619928</td>\n",
       "      <td>0.423513</td>\n",
       "      <td>0.161529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_zh</th>\n",
       "      <td>0.423398</td>\n",
       "      <td>0.449157</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.468428</td>\n",
       "      <td>0.432876</td>\n",
       "      <td>0.360268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361418</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336746</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "      <td>0.304984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326359</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "      <td>0.27196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   chf      gleu    meteor      bleu      nist   Cos_sim\n",
       "scores_cs      0.46223  0.427899  0.439964   0.46878  0.334652  0.368188\n",
       "scores_de     0.341149  0.310098   0.30805  0.346757  0.227363  0.290548\n",
       "scores_en_fi  0.611567  0.494636  0.491475  0.619928  0.423513  0.161529\n",
       "scores_en_zh  0.423398  0.449157  0.453092  0.468428  0.432876  0.360268\n",
       "scores_ru     0.361418  0.333536  0.336746  0.367581   0.26207  0.304984\n",
       "scores_zh     0.341207  0.317924  0.326359  0.351904  0.244659   0.27196"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check our Pearson correlation coefficients\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We increased to 0.161 from 0.150, but still a room for improvment - we will try to work with finBERT\n",
    "# BERT model that is trained and fine-tuned for finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
