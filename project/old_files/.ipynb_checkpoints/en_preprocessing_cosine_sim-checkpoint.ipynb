{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Import libraries\n",
    "import nltk.translate.chrf_score as chrf\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "import nltk.translate.gleu_score as gleu\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "try:\n",
    "  nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "  nltk.download('punkt')\n",
    "import nltk.translate.meteor_score as meteor\n",
    "import nltk.translate.nist_score as nist\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "  nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cs = pd.read_csv(\"corpus/cs-en/scores.csv\")\n",
    "scores_de = pd.read_csv(\"corpus/de-en/scores.csv\")\n",
    "scores_en_fi = pd.read_csv(\"corpus/en-fi/scores.csv\")\n",
    "scores_en_zh = pd.read_csv(\"corpus/en-zh/scores.csv\")\n",
    "scores_ru = pd.read_csv(\"corpus/ru-en/scores.csv\")\n",
    "scores_zh = pd.read_csv(\"corpus/zh-en/scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED - WAS LEFT HERE TO SHOW HOW CONTRACTIONS LOOK LIKE\n",
    "\n",
    "# contractions = { \n",
    "# \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "# \"aren't\": \"are not / am not\",\n",
    "# \"can't\": \"cannot\",\n",
    "# \"can't've\": \"cannot have\",\n",
    "# \"'cause\": \"because\",\n",
    "# \"could've\": \"could have\",\n",
    "# \"couldn't\": \"could not\",\n",
    "# \"couldn't've\": \"could not have\",\n",
    "# \"didn't\": \"did not\",\n",
    "# \"doesn't\": \"does not\",\n",
    "# \"don't\": \"do not\",\n",
    "# \"hadn't\": \"had not\",\n",
    "# \"hadn't've\": \"had not have\",\n",
    "# \"hasn't\": \"has not\",\n",
    "# \"haven't\": \"have not\",\n",
    "# \"he'd\": \"he had / he would\",\n",
    "# \"he'd've\": \"he would have\",\n",
    "# \"he'll\": \"he shall / he will\",\n",
    "# \"he'll've\": \"he shall have / he will have\",\n",
    "# \"he's\": \"he has / he is\",\n",
    "# \"how'd\": \"how did\",\n",
    "# \"how'd'y\": \"how do you\",\n",
    "# \"how'll\": \"how will\",\n",
    "# \"how's\": \"how has / how is / how does\",\n",
    "# \"I'd\": \"I had / I would\",\n",
    "# \"I'd've\": \"I would have\",\n",
    "# \"I'll\": \"I shall / I will\",\n",
    "# \"I'll've\": \"I shall have / I will have\",\n",
    "# \"I'm\": \"I am\",\n",
    "# \"I've\": \"I have\",\n",
    "# \"isn't\": \"is not\",\n",
    "# \"it'd\": \"it had / it would\",\n",
    "# \"it'd've\": \"it would have\",\n",
    "# \"it'll\": \"it shall / it will\",\n",
    "# \"it'll've\": \"it shall have / it will have\",\n",
    "# \"it's\": \"it has / it is\",\n",
    "# \"let's\": \"let us\",\n",
    "# \"ma'am\": \"madam\",\n",
    "# \"mayn't\": \"may not\",\n",
    "# \"might've\": \"might have\",\n",
    "# \"mightn't\": \"might not\",\n",
    "# \"mightn't've\": \"might not have\",\n",
    "# \"must've\": \"must have\",\n",
    "# \"mustn't\": \"must not\",\n",
    "# \"mustn't've\": \"must not have\",\n",
    "# \"needn't\": \"need not\",\n",
    "# \"needn't've\": \"need not have\",\n",
    "# \"o'clock\": \"of the clock\",\n",
    "# \"oughtn't\": \"ought not\",\n",
    "# \"oughtn't've\": \"ought not have\",\n",
    "# \"shan't\": \"shall not\",\n",
    "# \"sha'n't\": \"shall not\",\n",
    "# \"shan't've\": \"shall not have\",\n",
    "# \"she'd\": \"she had / she would\",\n",
    "# \"she'd've\": \"she would have\",\n",
    "# \"she'll\": \"she shall / she will\",\n",
    "# \"she'll've\": \"she shall have / she will have\",\n",
    "# \"she's\": \"she has / she is\",\n",
    "# \"should've\": \"should have\",\n",
    "# \"shouldn't\": \"should not\",\n",
    "# \"shouldn't've\": \"should not have\",\n",
    "# \"so've\": \"so have\",\n",
    "# \"so's\": \"so as / so is\",\n",
    "# \"that'd\": \"that would / that had\",\n",
    "# \"that'd've\": \"that would have\",\n",
    "# \"that's\": \"that has / that is\",\n",
    "# \"there'd\": \"there had / there would\",\n",
    "# \"there'd've\": \"there would have\",\n",
    "# \"there's\": \"there has / there is\",\n",
    "# \"they'd\": \"they had / they would\",\n",
    "# \"they'd've\": \"they would have\",\n",
    "# \"they'll\": \"they shall / they will\",\n",
    "# \"they'll've\": \"they shall have / they will have\",\n",
    "# \"they're\": \"they are\",\n",
    "# \"they've\": \"they have\",\n",
    "# \"to've\": \"to have\",\n",
    "# \"wasn't\": \"was not\",\n",
    "# \"we'd\": \"we had / we would\",\n",
    "# \"we'd've\": \"we would have\",\n",
    "# \"we'll\": \"we will\",\n",
    "# \"we'll've\": \"we will have\",\n",
    "# \"we're\": \"we are\",\n",
    "# \"we've\": \"we have\",\n",
    "# \"weren't\": \"were not\",\n",
    "# \"what'll\": \"what shall / what will\",\n",
    "# \"what'll've\": \"what shall have / what will have\",\n",
    "# \"what're\": \"what are\",\n",
    "# \"what's\": \"what has / what is\",\n",
    "# \"what've\": \"what have\",\n",
    "# \"when's\": \"when has / when is\",\n",
    "# \"when've\": \"when have\",\n",
    "# \"where'd\": \"where did\",\n",
    "# \"where's\": \"where has / where is\",\n",
    "# \"where've\": \"where have\",\n",
    "# \"who'll\": \"who shall / who will\",\n",
    "# \"who'll've\": \"who shall have / who will have\",\n",
    "# \"who's\": \"who has / who is\",\n",
    "# \"who've\": \"who have\",\n",
    "# \"why's\": \"why has / why is\",\n",
    "# \"why've\": \"why have\",\n",
    "# \"will've\": \"will have\",\n",
    "# \"won't\": \"will not\",\n",
    "# \"won't've\": \"will not have\",\n",
    "# \"would've\": \"would have\",\n",
    "# \"wouldn't\": \"would not\",\n",
    "# \"wouldn't've\": \"would not have\",\n",
    "# \"y'all\": \"you all\",\n",
    "# \"y'all'd\": \"you all would\",\n",
    "# \"y'all'd've\": \"you all would have\",\n",
    "# \"y'all're\": \"you all are\",\n",
    "# \"y'all've\": \"you all have\",\n",
    "# \"you'd\": \"you had / you would\",\n",
    "# \"you'd've\": \"you would have\",\n",
    "# \"you'll\": \"you shall / you will\",\n",
    "# \"you'll've\": \"you shall have / you will have\",\n",
    "# \"you're\": \"you are\",\n",
    "# \"you've\": \"you have\"\n",
    "# }\n",
    "\n",
    "# contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "# def expand_contractions(s, contractions = contractions):\n",
    "#     def replace(match):\n",
    "#         return contractions[match.group(0)]\n",
    "#     return contractions_re.sub(replace, s)\n",
    "\n",
    "# expand_contractions(\"ain't stop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with contractions as previous cell but pycontractes lib chooses a proper form\n",
    "# instead ambiguous \"you'd\" -> \"you had / you would\"\n",
    "\n",
    "from pycontractions import Contractions\n",
    "cont = Contractions('GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# prevents loading on first expand_texts call\n",
    "# usually takes time\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.\""
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = scores_de.translation\n",
    "a[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.Series(list(cont.expand_texts(a, precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how contracion expanding worked\n",
    "b[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As contraction extractions works really slow\n",
    "# it was done outside preprocessing function\n",
    "references_cs = pd.Series(list(cont.expand_texts(scores_cs['reference'], precise=True)))\n",
    "references_de = pd.Series(list(cont.expand_texts(scores_de['reference'], precise=True)))\n",
    "references_ru = pd.Series(list(cont.expand_texts(scores_ru['reference'], precise=True)))\n",
    "references_zh = pd.Series(list(cont.expand_texts(scores_zh['reference'], precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for transaltions\n",
    "translation_cs = pd.Series(list(cont.expand_texts(scores_cs['translation'], precise=True)))\n",
    "translation_de = pd.Series(list(cont.expand_texts(scores_de['translation'], precise=True)))\n",
    "translation_ru = pd.Series(list(cont.expand_texts(scores_ru['translation'], precise=True)))\n",
    "translation_zh = pd.Series(list(cont.expand_texts(scores_zh['translation'], precise=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.    ---->  \n",
      " it is now time to look at the needs of community legal centres and their client communities, and for the Palaszczuk government to invest in this important work.\n"
     ]
    }
   ],
   "source": [
    "# Check how expanding contractions worked\n",
    "print(scores_de['translation'][6], '   ---->  \\n',translation_de[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33260"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# release references and 'garbage collect' to free memory\n",
    "import gc\n",
    "del cont\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(series, pontuation = False):\n",
    "    processed_corpus = []\n",
    "    #stop_words = set(stopwords.words())\n",
    "    for i in tqdm(range(len(series))):\n",
    "        text = series[i]\n",
    "        \n",
    "        #remove tags\n",
    "        text = BeautifulSoup(text).get_text()\n",
    "        \n",
    "        #Remove punctuations\n",
    "        if pontuation==True:\n",
    "            text = re.sub(r'[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，？、~@#￥%……&*（）:：；《）《》“”()»〔〕-]+', ' ', text)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Convert to list from string\n",
    "        text = text.split()\n",
    "\n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        #text = [lem.lemmatize(word) for word in text if not word in stop_words] \n",
    "        text = \" \".join(text)\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefd1c7f8718408aa3e64cb6aa1591e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11585.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4228bf234c7436fab2bba4f24d2c7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21704.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679098372b274497b526e65fcdb99ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92f4cfc98014fce87d1dd2387f0b621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fa525552714e599f368ac836c5d50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17980.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c634d2cfa437451aa5359bffe41fb137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references_cs = preprocessing(references_cs)\n",
    "references_de = preprocessing(references_de)\n",
    "references_en_fi = preprocessing(scores_en_fi['reference'])\n",
    "references_en_zh = preprocessing(scores_en_zh['reference'])\n",
    "references_ru = preprocessing(references_ru)\n",
    "references_zh = preprocessing(references_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d30a4b348d453799c80c03cc5ba861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11585.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56346ad9dd7d45c8ba80e082e713af41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21704.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab536cc63a144b8faf6fbc29bae03a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30faa83899fc467a88488713bacddf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01a6af18ee849b085bc83a9d83f0b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17980.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f850b452290e4ad89c32e7e7ffee94a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translation_cs = preprocessing(translation_cs)\n",
    "translation_de = preprocessing(translation_de)\n",
    "translation_en_fi = preprocessing(scores_en_fi['translation'])\n",
    "translation_en_zh = preprocessing(scores_en_zh['translation'])\n",
    "translation_ru = preprocessing(translation_ru)\n",
    "translation_zh = preprocessing(translation_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS from your file text_mining1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrf_metric(translation, references):\n",
    "    chrf_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = chrf.sentence_chrf([translation[i]], references[i])\n",
    "        chrf_metric.append(row)\n",
    "    return chrf_metric\n",
    "\n",
    "chf_cs = chrf_metric(translation_cs, references_cs)\n",
    "scores_cs['chf'] = chf_cs\n",
    "chf_de = chrf_metric(translation_de, references_de)\n",
    "scores_de['chf'] = chf_de\n",
    "chf_en_fi = chrf_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['chf'] = chf_en_fi\n",
    "chf_en_zh = chrf_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['chf'] = chf_en_zh\n",
    "chf_ru = chrf_metric(translation_ru, references_ru)\n",
    "scores_ru['chf'] = chf_ru\n",
    "chf_zh = chrf_metric(translation_zh, references_zh)\n",
    "scores_zh['chf'] = chf_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gleu_metric(translation, references):\n",
    "    gleu_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = gleu.sentence_gleu([str(references[i]).split()], str(translation[i]).split(), min_len=1, max_len=2)\n",
    "        gleu_metric.append(row)\n",
    "    return gleu_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_cs = gleu_metric(translation_cs, references_cs)\n",
    "scores_cs['gleu'] = gleu_cs\n",
    "\n",
    "gleu_de = gleu_metric(translation_de, references_de)\n",
    "scores_de['gleu'] = gleu_de\n",
    "\n",
    "gleu_en_fi = gleu_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['gleu'] = gleu_en_fi\n",
    "\n",
    "gleu_en_zh = gleu_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['gleu'] = gleu_en_zh\n",
    "\n",
    "gleu_ru = gleu_metric(translation_ru, references_ru)\n",
    "scores_ru['gleu'] = gleu_ru\n",
    "\n",
    "gleu_zh = gleu_metric(translation_zh, references_zh)\n",
    "scores_zh['gleu'] = gleu_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteor_metric(translation, references):\n",
    "    meteor_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (meteor.meteor_score([translation[i]], references[i]))\n",
    "        meteor_metric.append(row)\n",
    "    return meteor_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_cs = meteor_metric(translation_cs, references_cs)\n",
    "scores_cs['meteor'] = meteor_cs\n",
    "\n",
    "meteor_de = meteor_metric(translation_de, references_de)\n",
    "scores_de['meteor'] = meteor_de\n",
    "\n",
    "meteor_en_fi = meteor_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['meteor'] = meteor_en_fi\n",
    "\n",
    "meteor_en_zh = meteor_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['meteor'] = meteor_en_zh\n",
    "\n",
    "meteor_ru = meteor_metric(translation_ru, references_ru)\n",
    "scores_ru['meteor'] = meteor_ru\n",
    "\n",
    "meteor_zh = meteor_metric(translation_zh, references_zh)\n",
    "scores_zh['meteor'] = meteor_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_metric(translation, references):\n",
    "    bleu_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (bleu.sentence_bleu([translation[i]], references[i]))\n",
    "        bleu_metric.append(row)\n",
    "    return bleu_metric\n",
    "\n",
    "bleu_cs = bleu_metric(translation_cs, references_cs)\n",
    "scores_cs['bleu'] = bleu_cs\n",
    "\n",
    "bleu_de = bleu_metric(translation_de, references_de)\n",
    "scores_de['bleu'] = bleu_de\n",
    "\n",
    "bleu_en_fi = bleu_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['bleu'] = bleu_en_fi\n",
    "\n",
    "bleu_en_zh = bleu_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['bleu'] = bleu_en_zh\n",
    "\n",
    "bleu_ru = bleu_metric(translation_ru, references_ru)\n",
    "scores_ru['bleu'] = bleu_ru\n",
    "\n",
    "bleu_zh = bleu_metric(translation_zh, references_zh)\n",
    "scores_zh['bleu'] = bleu_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nist_metric(translation, references):\n",
    "    nist_metric = []\n",
    "    for i in range(len(translation)):\n",
    "        row = (nist.sentence_nist([translation[i]], references[i],n=1))\n",
    "        nist_metric.append(row)\n",
    "    return nist_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_cs = nist_metric(translation_cs, references_cs)\n",
    "scores_cs['nist'] = nist_cs\n",
    "\n",
    "nist_de = nist_metric(translation_de, references_de)\n",
    "scores_de['nist'] = nist_de\n",
    "\n",
    "nist_en_fi = nist_metric(translation_en_fi, references_en_fi)\n",
    "scores_en_fi['nist'] = nist_en_fi\n",
    "\n",
    "nist_en_zh = nist_metric(translation_en_zh, references_en_zh)\n",
    "scores_en_zh['nist'] = nist_en_zh\n",
    "\n",
    "nist_ru = nist_metric(translation_ru, references_ru)\n",
    "scores_ru['nist'] = nist_ru\n",
    "\n",
    "nist_zh = nist_metric(translation_zh, references_zh)\n",
    "scores_zh['nist'] = nist_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(scores):\n",
    "    chf = scores['z-score'].corr(scores['chf'])\n",
    "    gleu = scores['z-score'].corr(scores['gleu'])\n",
    "    meteor = scores['z-score'].corr(scores['meteor'])\n",
    "    bleu = scores['z-score'].corr(scores['bleu'])\n",
    "    nist = scores['z-score'].corr(scores['nist'])\n",
    "    #ribes = scores['z-score'].corr(scores['ribes'])\n",
    "    corr = [chf, gleu, meteor, bleu, nist]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cs = corr(scores_cs)\n",
    "corr_de = corr(scores_de)\n",
    "corr_en_fi = corr(scores_en_fi)\n",
    "corr_en_zh = corr(scores_en_zh)\n",
    "corr_ru = corr(scores_ru)\n",
    "corr_zh = corr(scores_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['chf', 'gleu', 'meteor', 'bleu', 'nist']\n",
    "data = []\n",
    "index = ['scores_cs', 'scores_de', 'scores_en_fi', 'scores_en_zh', 'scores_ru', 'scores_zh']\n",
    "correlation = pd.DataFrame(data, columns=columns, index = index)\n",
    "correlation.iloc[0] = corr_cs\n",
    "correlation.iloc[1] = corr_de\n",
    "correlation.iloc[2] = corr_en_fi\n",
    "correlation.iloc[3] = corr_en_zh\n",
    "correlation.iloc[4] = corr_ru\n",
    "correlation.iloc[5] = corr_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.462233</td>\n",
       "      <td>0.427893</td>\n",
       "      <td>0.440004</td>\n",
       "      <td>0.468779</td>\n",
       "      <td>0.334652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.30805</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_fi</th>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.494636</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>0.619928</td>\n",
       "      <td>0.423513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_en_zh</th>\n",
       "      <td>0.435736</td>\n",
       "      <td>0.0296857</td>\n",
       "      <td>0.0231177</td>\n",
       "      <td>0.433811</td>\n",
       "      <td>0.444757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361417</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336745</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341204</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326358</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   chf       gleu     meteor      bleu      nist\n",
       "scores_cs     0.462233   0.427893   0.440004  0.468779  0.334652\n",
       "scores_de     0.341149   0.310098    0.30805  0.346757  0.227363\n",
       "scores_en_fi  0.611567   0.494636   0.491475  0.619928  0.423513\n",
       "scores_en_zh  0.435736  0.0296857  0.0231177  0.433811  0.444757\n",
       "scores_ru     0.361417   0.333536   0.336745  0.367581   0.26207\n",
       "scores_zh     0.341204   0.317924   0.326358  0.351904  0.244659"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline correlation\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cosine similarities for sentences using word embeddings\n",
    " - pre-trained Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english stopwords\n",
    "english_stop_words = set(stopwords.words('english')).union(STOP_WORDS)\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "# Commented\n",
    "# nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing - delete stopwords, lemmatize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "def words_count(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # delete stop words\n",
    "    tokens = list(filter(lambda x: x not in english_stop_words, tokens))\n",
    "    \n",
    "    # lemmatize the words WordNetLemmatizer from nlkt\n",
    "    # this made Pearson correlation lower - so was commented\n",
    "    # tokens = list(map(lambda x: nltk.WordNetLemmatizer().lemmatize(x), tokens))\n",
    "    \n",
    "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "    #tokens = list(map(lambda x:  nlp(x), tokens))\n",
    "\n",
    "    return list(Counter(tokens).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe (Global Vectors for Word Representation) word vectors. \n",
    "# It has 6B tokens, 400K vocab, (300d version).\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "    return word_to_vec_map\n",
    "# Download glove.6B.300d.txt to a folder with this jupiter notebook\n",
    "word_to_vec_map = read_glove_vecs('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cs['Cosine_similarity'], scores_de['Cosine_similarity'] = 0, 0\n",
    "scores_ru['Cosine_similarity'], scores_zh['Cosine_similarity'] = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists with languages to work with (that translated to -> en)\n",
    "\n",
    "ref_list = [references_cs, references_de, references_ru, references_zh]\n",
    "trans_list = [translation_cs, translation_de, translation_ru, translation_zh]\n",
    "scores = [scores_cs, scores_de, scores_ru, scores_zh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 0th document\n",
      "working on 1000th document\n",
      "working on 2000th document\n",
      "working on 3000th document\n",
      "working on 4000th document\n",
      "working on 5000th document\n",
      "working on 6000th document\n",
      "working on 7000th document\n",
      "working on 8000th document\n",
      "working on 9000th document\n",
      "working on 10000th document\n",
      "working on 11000th document\n",
      "working on 0th document\n",
      "working on 1000th document\n",
      "working on 2000th document\n",
      "working on 3000th document\n",
      "working on 4000th document\n",
      "working on 5000th document\n",
      "working on 6000th document\n",
      "working on 7000th document\n",
      "working on 8000th document\n",
      "working on 9000th document\n",
      "working on 10000th document\n",
      "working on 11000th document\n",
      "working on 12000th document\n",
      "working on 13000th document\n",
      "working on 14000th document\n",
      "working on 15000th document\n",
      "working on 16000th document\n",
      "working on 17000th document\n",
      "working on 18000th document\n",
      "working on 19000th document\n",
      "working on 20000th document\n",
      "working on 21000th document\n",
      "working on 0th document\n",
      "working on 1000th document\n",
      "working on 2000th document\n",
      "working on 3000th document\n",
      "working on 4000th document\n",
      "working on 5000th document\n",
      "working on 6000th document\n",
      "working on 7000th document\n",
      "working on 8000th document\n",
      "working on 9000th document\n",
      "working on 10000th document\n",
      "working on 11000th document\n",
      "working on 12000th document\n",
      "working on 13000th document\n",
      "working on 14000th document\n",
      "working on 15000th document\n",
      "working on 16000th document\n",
      "working on 17000th document\n",
      "working on 0th document\n",
      "working on 1000th document\n",
      "working on 2000th document\n",
      "working on 3000th document\n",
      "working on 4000th document\n",
      "working on 5000th document\n",
      "working on 6000th document\n",
      "working on 7000th document\n",
      "working on 8000th document\n",
      "working on 9000th document\n",
      "working on 10000th document\n",
      "working on 11000th document\n",
      "working on 12000th document\n",
      "working on 13000th document\n",
      "working on 14000th document\n",
      "working on 15000th document\n",
      "working on 16000th document\n",
      "working on 17000th document\n",
      "working on 18000th document\n",
      "working on 19000th document\n",
      "working on 20000th document\n",
      "working on 21000th document\n",
      "working on 22000th document\n",
      "working on 23000th document\n",
      "working on 24000th document\n",
      "working on 25000th document\n",
      "working on 26000th document\n"
     ]
    }
   ],
   "source": [
    "# For every language pair that translated to english:\n",
    "# cs, de, ru, zh\n",
    "\n",
    "for doc in range(len(scores)):\n",
    "    ref_embeddings = np.zeros((len(scores[doc]),300))\n",
    "    tr_embeddings = np.zeros((len(scores[doc]),300))\n",
    "    \n",
    "    norms_doc_embeddings_tr = np.zeros((len(trans_list[doc]),1))\n",
    "    norms_doc_embeddings_ref = np.zeros((len(ref_list[doc]),1))\n",
    "    \n",
    "    Cosine_similarity = []\n",
    "    \n",
    "    for i in range(len(trans_list[doc])):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"working on \"+str(i)+\"th document\")\n",
    "        words_freq = words_count(trans_list[doc][i])\n",
    "        doc_embedding_vec = np.zeros(word_to_vec_map[\"a\"].shape)\n",
    "        num_words = 0\n",
    "        for word_freq in words_freq:\n",
    "            word = word_freq[0]\n",
    "            freq = word_freq[1]\n",
    "            try:\n",
    "                #adding word embeddings for each word in the document\n",
    "                doc_embedding_vec += (word_to_vec_map[word] * freq)\n",
    "                num_words += freq\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            # doing average\n",
    "            doc_embedding_vec /= num_words\n",
    "        except:\n",
    "            print(\"divide by zero encountered for article at index \"+str(i))\n",
    "            continue\n",
    "        norms_doc_embeddings_tr[i,:] = np.sqrt(np.dot(doc_embedding_vec,doc_embedding_vec))\n",
    "        tr_embeddings[i,:] = doc_embedding_vec\n",
    "        \n",
    "        \n",
    "        \n",
    "        words_freq = words_count(ref_list[doc][i])\n",
    "        doc_embedding_vec = np.zeros(word_to_vec_map[\"a\"].shape)\n",
    "        num_words = 0\n",
    "        for word_freq in words_freq:\n",
    "            word = word_freq[0]\n",
    "            freq = word_freq[1]\n",
    "            try:\n",
    "                #adding word embeddings for each word in the document\n",
    "                doc_embedding_vec += (word_to_vec_map[word] * freq)\n",
    "                num_words += freq\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            # doing average\n",
    "            doc_embedding_vec /= num_words\n",
    "        except:\n",
    "            print(\"divide by zero encountered for article at index \"+str(i))\n",
    "            continue\n",
    "        norms_doc_embeddings_ref[i,:] = np.sqrt(np.dot(doc_embedding_vec,doc_embedding_vec))\n",
    "        ref_embeddings[i,:] = doc_embedding_vec\n",
    "    \n",
    "        # Calculate cosine similarity using dot product of two vectors\n",
    "        # and norms of vectors\n",
    "        \n",
    "        #  The cosine similarity depends on the angle between  vectors .\n",
    "        #  If  vectors  are very similar, their cosine similarity will be close to 1\n",
    "        norm_prods = norms_doc_embeddings_ref[i] * norms_doc_embeddings_tr[i]\n",
    "        dot_prod = np.dot(ref_embeddings[i], tr_embeddings[i].reshape(300,1))\n",
    "        cosine_similarity = dot_prod / norm_prods\n",
    "\n",
    "        Cosine_similarity.append(float(cosine_similarity))\n",
    "    \n",
    "    scores[doc]['Cosine_similarity'] = pd.Series(Cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_new(scores):\n",
    "    chf = scores['z-score'].corr(scores['chf'])\n",
    "    gleu = scores['z-score'].corr(scores['gleu'])\n",
    "    meteor = scores['z-score'].corr(scores['meteor'])\n",
    "    bleu = scores['z-score'].corr(scores['bleu'])\n",
    "    nist = scores['z-score'].corr(scores['nist'])\n",
    "    cos = scores['z-score'].corr(scores['Cosine_similarity'])\n",
    "    corr = [chf, gleu, meteor, bleu, nist, cos]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cs_new = corr_new(scores_cs)\n",
    "corr_de_new = corr_new(scores_de)\n",
    "corr_ru_new= corr_new(scores_ru)\n",
    "corr_zh_new = corr_new(scores_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['chf', 'gleu', 'meteor', 'bleu', 'nist', 'Cos_sim']\n",
    "data = []\n",
    "index = ['scores_cs', 'scores_de', 'scores_ru', 'scores_zh']\n",
    "correlation = pd.DataFrame(data, columns=columns, index = index)\n",
    "correlation.iloc[0] = corr_cs_new\n",
    "correlation.iloc[1] = corr_de_new\n",
    "# Deleted rows for en-zh, en-fi\n",
    "correlation.iloc[2] = corr_ru_new\n",
    "correlation.iloc[3] = corr_zh_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "      <th>Cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.462233</td>\n",
       "      <td>0.427893</td>\n",
       "      <td>0.440004</td>\n",
       "      <td>0.468779</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.368188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.30805</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "      <td>0.290548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361417</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336745</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "      <td>0.304984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341204</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326358</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "      <td>0.27196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                chf      gleu    meteor      bleu      nist   Cos_sim\n",
       "scores_cs  0.462233  0.427893  0.440004  0.468779  0.334652  0.368188\n",
       "scores_de  0.341149  0.310098   0.30805  0.346757  0.227363  0.290548\n",
       "scores_ru  0.361417  0.333536  0.336745  0.367581   0.26207  0.304984\n",
       "scores_zh  0.341204  0.317924  0.326358  0.351904  0.244659   0.27196"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This correlation different from next one cells (I changed some params in words_count\n",
    "# and checked only cos_sim column changes, other metrics are the same):\n",
    "# Cos_sim was calculated with such changes in words_count function:\n",
    "# without lemmatization, punctuation and stopwords were deleted\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT CELLS SHOW LOWER VALUES OF PEARSON CORRELATION WITH COS_SIM\n",
    "\n",
    "\n",
    "They were left to show that the values for cos_sim in previous cell are better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "      <th>Cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.462233</td>\n",
       "      <td>0.427893</td>\n",
       "      <td>0.440004</td>\n",
       "      <td>0.468779</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.365164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.30805</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "      <td>0.287869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361417</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336745</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "      <td>0.30439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341204</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326358</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "      <td>0.268226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                chf      gleu    meteor      bleu      nist   Cos_sim\n",
       "scores_cs  0.462233  0.427893  0.440004  0.468779  0.334652  0.365164\n",
       "scores_de  0.341149  0.310098   0.30805  0.346757  0.227363  0.287869\n",
       "scores_ru  0.361417  0.333536  0.336745  0.367581   0.26207   0.30439\n",
       "scores_zh  0.341204  0.317924  0.326358  0.351904  0.244659  0.268226"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This correlation different from previous (only cos_sim different):\n",
    "# Cos_sim was calculated with such changes in words_count function:\n",
    "# with lemmatization (nlkt),  punctuation and stopwords were deleted\n",
    "correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chf</th>\n",
       "      <th>gleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>bleu</th>\n",
       "      <th>nist</th>\n",
       "      <th>Cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores_cs</th>\n",
       "      <td>0.462233</td>\n",
       "      <td>0.427893</td>\n",
       "      <td>0.440004</td>\n",
       "      <td>0.468779</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.316055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_de</th>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.30805</td>\n",
       "      <td>0.346757</td>\n",
       "      <td>0.227363</td>\n",
       "      <td>0.256165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_ru</th>\n",
       "      <td>0.361417</td>\n",
       "      <td>0.333536</td>\n",
       "      <td>0.336745</td>\n",
       "      <td>0.367581</td>\n",
       "      <td>0.26207</td>\n",
       "      <td>0.259776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scores_zh</th>\n",
       "      <td>0.341204</td>\n",
       "      <td>0.317924</td>\n",
       "      <td>0.326358</td>\n",
       "      <td>0.351904</td>\n",
       "      <td>0.244659</td>\n",
       "      <td>0.223975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                chf      gleu    meteor      bleu      nist   Cos_sim\n",
       "scores_cs  0.462233  0.427893  0.440004  0.468779  0.334652  0.316055\n",
       "scores_de  0.341149  0.310098   0.30805  0.346757  0.227363  0.256165\n",
       "scores_ru  0.361417  0.333536  0.336745  0.367581   0.26207  0.259776\n",
       "scores_zh  0.341204  0.317924  0.326358  0.351904  0.244659  0.223975"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This correlation different from previous (only cos_sim different):\n",
    "# Cos_sim was calculated with such changes in words_count  function:\n",
    "# with lemmatization (nlkt), stopwords were left in text, punctuation deleted\n",
    "correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
